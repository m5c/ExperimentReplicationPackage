{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RESTify Experiment Replication Package","text":"<p>All you need to replicate and inspect our findings, build atop of our raw data, and reuse the tools we built for this study.</p> <p>The simplest way to replicate our study findings, is the prepared docker image. Within just a few minutes you can power up your own Jupyter Notebook and replicate all statics and figures printed in the paper submission. You only need docker and a browser.</p>"},{"location":"#about","title":"About","text":"<p>This webpage servers as entry point for the artifact submission of our MODELS 2024 conference contribution.</p> <ul> <li>Main purpose of this replication package is to allow fast and independent replication of all our results and interpretations.</li> <li>We carefully documented all our methodology, and automated our analysis. E.g. all paper figures are generated from raw data, and we provide you the means to conveniently validate correctness of our findings on your local machine.  </li> <li>Furthermore we publish all study material and tools used to conduct the study, so you can easily replicate the experiment, or reuse parts of our material for follow-up research.</li> </ul>"},{"location":"#package-content","title":"Package Content","text":"<p>This section serves as starting point, where we present the various components of our accompanying artifact submission. Each item on the left navigation bar opens a dedicated page for detailed information of a specific artifact component.</p>"},{"location":"#replication","title":"Replication","text":"<ul> <li>Result Replication: Center-piece of our replication package is a Jupyter Notebook that takes you through the data analysis and replicates all paper figures on your machine.<ul> <li>We offer various levels of inspection, from static prerendered notebook, to dockerized local replication, to in-depth analysis of the implementation in PyCharm.</li> <li>For the replication we provide you with the raw data collected throughout the experiment. Then you crunch it locally and replicate the findings, all using convenient scripts.</li> <li>The data analysis was coded for easy inspection and transparency. You can at any point in time inspect the well-documented codenbase, and verify our implementation.</li> </ul> </li> </ul>"},{"location":"#material","title":"Material","text":"<p>In addition to replication of our findings, we provide everything needed to replicate the experiment itself, i.e. we publish the sources for all material and tools involved in performing the experiment:</p> <ul> <li>Recruitment Material: Sources and rendered HTML version of the webpage used for participant recruitment.</li> <li>Group Allocation Algo: Source code and documentation of the algorithm implemented to create a balanced participant groups (comparable skillsets per group).</li> <li>Task Instruction Material: Sources and rendered HTML version with textual and video task instructions for all four control groups.</li> <li>Sample Legacy Applications (Objects): The sample applications used task for training or actual conversion task.</li> </ul>"},{"location":"#data-and-tools","title":"Data and Tools","text":"<ul> <li>Submission Correctness Evaluation Tool: Sources and documentation of the analyzer tool that we implemented to assess correctness of participant submissions. A reusable tool to test REST APIs against a predefined interface and produce correctness reports.</li> <li>Raw Experiment Collected Data: Raw data collected throughout the experiment: CSV and textual summary files that conclude data extracted from video material viewing, such as transcript of observations and time measurements for the individual conversion tasks.</li> </ul>"},{"location":"about/","title":"About","text":"<p>This is the replication package of the RESTify controlled Experiment.</p> <ul> <li>PI: Maximilian Schiedermeier</li> <li>Academic Supervisors: Bettina Kemme, J\u00f6rg Kienzle</li> </ul>"},{"location":"allocation/","title":"Group Allocation","text":""},{"location":"allocation/#source-code","title":"Source Code","text":"<p>The source code of the MiniMax implementation and documentation is available on GitHub.</p>"},{"location":"allocation/#explanations","title":"Explanations","text":"<p>We implemented a custom MiniMax Heuristic, to assess the participant's skill profiles and create balanced experiment groups.  </p> <ul> <li>To maintain transparency we provide source code and documentation of the heuristic used to create the participant allocations.</li> <li>The program parses all provided self assessment forms (see corresponding entry of replication bundle which they provided during recruitment, and then performs a MiniMax search for the fairest group allocation.</li> <li>A side product of this software is the generation of personalized emails for following communication with the individual participants. </li> </ul> <p>Since the purpose of this program is to select of all applicants and provide a mapping from their legal names to pseudonyms, it can be only executed with access to the participant details. The latter we cannot release for legal reasons.</p>"},{"location":"analyzer/","title":"Submission Analyzer","text":"<p>We wrote an open source tool to test migrated applications: Analyzer Sources and Documentation on GitHub</p>"},{"location":"analyzer/#how-it-works","title":"How it works","text":"<ul> <li>You download the raw participant submission data from this webpage</li> <li>You clone the analyzer and run it. It produces a CSV test report.</li> <li>You compare the test report against the one we use for our own statistical analysis, so you see out statistical analysis actually operates on the real data.</li> </ul>"},{"location":"data/","title":"Data. Like... all of it. Kind of.","text":"<p>All data we collected. </p> <p>In some cases we had to anonymize, to protect participant identity. But you can still replicate all results. Cheers.</p>"},{"location":"data/#video-observations","title":"Video Observations","text":"<p>We collected more than 72 hours of video onscreen recordings throughout the experiment. Participants were asked to avoid capturing personal information or identifiers. Unfortunately this request was widely ignored. To preserve participant anonymity, we cannot provide the original video material. Bummer. We can however provide all information, extracted from the video material.</p> <ul> <li>Task solving and task preparation times: We measured how much time every participant spent on the actual project conversion tasks and the instructions and provide precise information on their time spendings.</li> <li>Task deviations, difficulties, remarkable observations: We provide for each participant and each refactoring task a transcript of all noteworthy events. This includes problems with specific task phases, problems with software used, even information on their task solving activity itself.</li> <li>Participant methodology feedback preferences</li> </ul> <p>Here's the actual data, in various file formats:</p> <ul> <li>Mac Numbers File</li> <li>Microsoft Excel File</li> <li>CSV File</li> </ul>"},{"location":"data/#participant-feedback","title":"Participant Feedback","text":"<p>Participant issued comments on the study, issues, preferences after study conclusion.</p> <p>We crunched it all for you, in various file formats:</p> <ul> <li>Mac Numbers File</li> <li>Microsoft Excel File</li> <li>CSV File </li> </ul> <p>We also created an informal meta summary of most common issues observed in task solving.</p>"},{"location":"data/#code-model-submissions","title":"Code / Model Submissions","text":"<p>Everything the participants actively produced on their endevour to migrate the provided legacy applications:</p> <ul> <li>All raw code and all models (anonymized) (and the corresponding generated code), provided by participants.</li> <li>All patched code and all models (anonymized) (and the corresponding generated code), provided by participants.</li> </ul> <p>We patched some submissions before testing, i.e. in some cases a minimal configuration mistake prevented testing. See our publication for the details on the patches performed.</p>"},{"location":"data/#participant-test-results","title":"Participant Test Results","text":"<p>We tested all submissions against the requested REST API interface description.</p> <p>Here's a CSV with all the results. You can reproduce the CSV using the above code / model submissions and our provided tester.</p>"},{"location":"material/","title":"Training and Task Material","text":"<p>The original task training and task instructions material. One for each of the four experiment groups:</p> <ul> <li>Red Group</li> <li>Green Group</li> <li>Blue Group</li> <li>Yellow Group</li> </ul> <p>Task context and order changes, depending on which group a participant was allocated to.</p>"},{"location":"objects/","title":"Objects","text":"<p>The study contains three sample  lecagy applications for migration to REST.</p>"},{"location":"objects/#the-zoo","title":"The Zoo","text":"<p>The zoo is only used for subject training. It is smaller than the applications used for the migration tasks. Zoo Legacy Sources on GitHub</p>"},{"location":"objects/#the-bookstore","title":"The BookStore","text":"<p>Legacy application No.1: A simple e-commerce application. BookStore Legacy Sources on GitHub</p>"},{"location":"objects/#xox","title":"Xox","text":"<p>Legacy application No.2: A simple turn based board game classic. Xox Legacy Sources on GitHub</p>"},{"location":"recruitment/","title":"Recruitment Webpage","text":"<p>Here you find anonymized references to the material used for recruitment:</p> <ul> <li>Original Recruitment webpage</li> <li>The auto assessment form used to assess preliminary participant skills</li> </ul>"},{"location":"replication/","title":"Result Replication","text":"<p>All statistical figures and listings in our paper submission were created programmatically. The code to produce them is publicly available and well documented.</p> <p>On the dedicated project page, we provide:</p> <ul> <li>The raw data (but you get that also here, for your convenience ;) )</li> <li>The code used for statistical analysis</li> <li>Instructions on how to very rapidly replicate / reproduce our results</li> </ul> <p>Spoiler alert: we use docker + jupyter, so you don't need to install anything.</p>"},{"location":"replication/#where-to-start","title":"Where to Start","text":"<p>We provide several access points, for fast to thorough replication:</p> <ul> <li>Static, rendered Jupyter Notebook: You can inspect a static one-shot render of our Notebook on GitHub. You only need a browser. Please note that the anonymization process damaged some of the integrated figures. We apologize for the inconvenience.</li> <li>Dynamic, using a docker configuration. See install instructions for Docker.</li> <li>Dynamic, your own Jupyter Notebook. See install instructions for native Jupyter</li> <li>Dynamic, using the raw python sources, and an IDE. See install instructions for PyCharm</li> </ul>"}]}